\chapter{Tre algoritmi per ordinare con informazione parziale}

% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ %
\section{Ordinamento con informazione parziale} 
\begin{definition}
	Sia \(P=\left(V,\le_{P}\right)\) un insieme parzialmente ordinato di cardinalit\`a \(n\). Diciamo che un ordine totale \(\le\) \`e una \emph{estensione lineare} di \(\le_{P}\) se \((\forall v_i, v_j\in V)\;v_i\le_{P} v_j\Rightarrow v_i\le v_j\). Denotiamo inoltre con \(e(P)\) il numero di estensioni lineari di \(P\). 
\end{definition}
\begin{definition}
	Sia \(P=(V,\le_{P})\) un insieme parzialmente ordinato di cardinalit\`a \(n\). Il \emph{problema dell'ordinamento con informazione parziale} consiste nel determinare una estensione lineare \(\le\) fissata ma ignota per mezzo di domande del tipo ``\`e vero che \(v_i\le v_j\) ?'', detti \emph{confronti}. 
\end{definition}

Tale problema fu originariamente posto da Fredman nel 1976 \cite{Fredman1976}. Sempre Fredman dimostrò l'esistenza di un algoritmo che lo risolva compiendo \(\log{e(P)} + 2n\) confronti, il quale tuttavia richiede tempo di esecuzione superpolinomiale. Nel 1984 Kahn e Saks dimostrarono l'esistenza di un algoritmo che compia \(O(\log{e(P)})\) confronti \cite{Kahn1984}. Essi mostrarono infatti che esiste sempre un confronto tale che le estensioni lineari per cui la risposta sia affermativa siano una parte compresa fra \(3/11\) e \(8/11\) del totale.\footnote{Questo enunciato è un rilassamento della congettura \(1/3-2/3\), indipendentemente posta da Kislitsyn nel 1968 \cite{Kislitsyn1968}, da Fredman nel 1975 e da Linial nel 1984 \cite{Linial1984}.}

È evidente che siano necessari \(\Omega(\log{e(P)})\) confronti: dobbiamo infatti discriminare fra \(e(P)\) possibili risultati, e ogni confronto ci fornisce esattamente un bit di informazione. Servono quindi \(\log{e(P)}\) bit, da cui il precedente limite inferiore.

La questione dell'esistenza di un algoritmo che compia \(O(\log{e(P)})\) confronti e che richieda tempo polinomiale rimase aperta fino al 1995, quando un articolo di Kahn e Kim evidenziò il collegamento esistente fra l'entropia del grafo associato a \(P\) ed \(e(P)\) \cite{Kahn1995}. Il seguente teorema afferma ad esempio che \(nH(\overline{P})=\Theta(\log{e(P)})\). 
\begin{theorem}
	[Kahn, Kim] \label{kktheorem} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora vale
	\[\log{e(P)}\le nH\left(\overline{P}\right)\le\min\left\{\log{e(P)}+\log{e\cdot n},\; c\log{e(P)}\right\},\]
	dove \(c=1+7\log{e}\approx 11.1\). 
\end{theorem}
L'algoritmo di Kahn e Kim calcola inizialmente l'entropia del grafo associato a \(P\). Successivamente stima la variazione dell'entropia dei grafi associati agli ordini parziali \(P'\), ottenuti da \(P\) aggiungendo il risultato di un confronto. Viene quindi selezionato quel confronto che avvicini maggiormente l'entropia a \(\log{n}\), entropia del grafo completo, associato all'insieme totalmente ordinato. Ad ogni passo è dunque necessario il calcolo dell'entropia di un grafo, un problema di minimizzazione su un insieme convesso per l'equazione \ref{eq:entropythree}. Possiamo quindi applicare il metodo dell'ellissoide, ottenendo un algoritmo polinomiale ma non utile nella pratica.

In un recente articolo Cardinal et al. hanno proposto tre algoritmi che non richiedono il calcolo esatto dell'entropia di alcun grafo, ma sfruttano la versione approssimata presentata nel teorema \ref{greedypoint} \cite{Cardinal2010}. Questo consente di ottenere algoritmi che richiedono \(O(\log{e(P)})\) confronti e che sono contemporaneamente polinomiali e pratici. In questo capitolo andremo ad esporre tali algoritmi.

Concludiamo questa sezione con una versione più precisa della stima superiore del teorema \ref{kktheorem}, un risultato che sarà utile nel seguito. 
\begin{theorem}
	[Cardinal, Fiorini, Joret, Jungers, Munro] \label{cfjjmtheorem} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora vale
	\[nH\left(\overline{P}\right)\le 2\log{e(P)}.\]
\end{theorem}
\begin{proof}
	La dimostrazione procede per induzione su \(n\), e, per \(n\) fissato, sul numero di elementi inconfrontabili di \(P\). Essendo la tesi banalmente vera per \(n=1\) supponiamo \(n\ge 2\). Sia \(x\in \mathbb{R}_{+}^{V}\) un vettore che realizzi il minimo dell'entropia. Sia inoltre \(\left\{\left(y_{v^-},y_{v^+}\right)\right\}_{v\in V}\) la corrispondente collezione di intervalli. Sia infine \(a\in V\) tale che \(y_{a^+}\) sia massimo. Se \(a\) fosse confrontabile con tutti gli elementi di \(V\) avremmo per ipotesi induttiva che
	\[nH\left(\overline{P}\right)=(n-1)H\left(\overline{P-a}\right)\le 2\log{e(P-a)=2\log{e(P)}}.\]
	Sia allora \(b\) non confrontabile con \(a\) e tale inoltre che \(y_{b^+}\) sia massimo. Per come abbiamo scelto \(a\) deve per forza valere \(y_{b^+}\le y_{a^+}\). In realtà vale l'uguaglianza: supponiamo infatti per assurdo che \(y_{b^+}<y_{a^+}\), ed estendiamo a destra l'intervallo corrispondente a \(b\) di \(y_{a^+}-y_{b^+}\). Questa nuova collezione di intervalli è ancora consistente con \(P\), ma il punto \(x'\in \mathbb{R}_{+}^{V}\) da essa definito realizzerebbe un valore dell'entropia più piccolo del minimo. Abbiamo infatti
	\[-\frac{1}{n}\sum_{v\in V}{\log{x'_{v}}}=-\frac{1}{n}\sum_{v\in V}{\log{x_{v}}}+\frac{1}{n}\left(\log{x_b}-\log{x'_{b}}\right)<-\frac{1}{n}\sum_{v\in V}{\log{x_v}},\]
	contro l'ipotesi che \(x\) realizzi il minimo dell'entropia. A meno di scambiare \(a\) e \(b\) possiamo ora supporre che \(x_a\ge x_b\). Il nostro obiettivo ora è definire due nuove famiglie di intervalli
	\[\left\{\left(y_{v^-}^1, y_{v^+}^1\right)\right\}_{v\in V}\qquad\mbox{ e }\qquad\left\{\left(y_{v^-}^2, y_{v^+}^2\right)\right\}_{v\in V}\]
	tali che gli insiemi parzialmente ordinati \(P_1\) e \(P_2\) ad esse associati estendano \(P\), e tali inoltre che le quantità \(e(P_1)\) ed \(e(P_2)\) varino in modo controllato. Per fare questo poniamo
	\[\lambda=\frac{x_b}{x_a},\]
	compreso fra \(0\) e \(1\) per come abbiamo scelto \(a\) e \(b\). Poniamo inoltre
	\[\alpha_1= 
	\begin{cases}
		\frac{1}{1-\lambda} & \mbox{se } \lambda\le\frac{1}{2} \\
		2 & \mbox{altrimenti} 
	\end{cases}
	\qquad\mbox{ e }\qquad \beta_1= 
	\begin{cases}
		1 & \mbox{se } \lambda\le\frac{1}{2} \\
		2\lambda & \mbox{altrimenti} 
	\end{cases}
	\]
	e infine
	\[\alpha_2=\frac{2}{\lambda}\qquad\mbox{ e }\qquad\beta_2=2.\]
	Allora la famiglia di intervalli \(\left\{\left(y_{v^-}^1, y_{v^+}^1\right)\right\}_{v\in V}\) coincide con \(\left\{\left(y_{v^-}, y_{v^+}\right)\right\}_{v\in V}\) tranne per 
	\begin{align}
		y_{a^+}^1 &= y_{a^-} + \frac{x_a}{\alpha_1} \nonumber \\
		y_{b^-}^1 &= y_{b^+} - \frac{x_b}{\beta_1}, \nonumber 
	\end{align}
	e analogamente \(\left\{\left(y_{v^-}^2, y_{v^+}^2\right)\right\}_{v\in V}\) coincide con \(\left\{\left(y_{v^-}, y_{v^+}\right)\right\}_{v\in V}\) eccetto per 
	\begin{align}
		y_{a^-}^2 &= y_{a^+} - \frac{x_a}{\alpha_2} \nonumber \\
		y_{b^+}^2 &= y_{b^-} + \frac{x_b}{\beta_2}. \nonumber 
	\end{align}
	Siano rispettivamente \(P_1\) e \(P_2\) gli insiemi parzialmente ordinati definiti dalla prima e dalla seconda famiglia di intervalli. Allora esiste un indice \(i\in\left\{1,2\right\}\) tale che 
	\begin{equation}
		\label{appendixlemma} \frac{e(P_i)}{e(P)}\le\frac{1}{\sqrt{\alpha_i\beta_i}}. 
	\end{equation}
	Questo fatto verrà dimostrato in appendice. Assumendo che esista un tale \(i\) sia \(x'\in\mathbb{R}_{+}^V\) il vettore definito dalla corrispondente famiglia di intervalli. Abbiamo allora che
	\[H(P_i)\le-\frac{1}{n}\sum_{v\in V}{\log{x'_v}=-\frac{1}{n}\sum_{v\in V}{\log{x_v}}+\frac{1}{n}\log{\alpha_i}+\frac{1}{n}\log{\beta_i}},\]
	dunque
	\[nH(P_i)\le nH(P)+\log{\alpha_i\beta_i}.\]
	Possiamo ora concludere. Per il teorema \ref{lovasztheorem} e per la disuguaglianza appena dimostrata possiamo scrivere 
	\begin{align}
		nH(\overline{P}) &= n\log{n}-nH(P) \nonumber \\
		&\le n\log{n}-nH(P_i)+\log{\alpha_i\beta_i} \nonumber \\
		&= nH(\overline{P_i})+\log{\alpha_i\beta_i}, \nonumber 
	\end{align}
	mentre per ipotesi induttiva e per la disuguaglianza \ref{appendixlemma} abbiamo 
	\begin{align}
		nH(\overline{P_i})+\log{\alpha_i\beta_i} &\le 2\log{e(P_i)}+\log{\alpha_i\beta_i} \nonumber \\
		&\le 2\log{\frac{e(P)}{\sqrt{\alpha_i\beta_i}}}+\log{\alpha_i\beta_i} \nonumber \\
		&\le 2\log{e(P)}, \nonumber 
	\end{align}
	cioè la tesi.\qed 
\end{proof}

% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ %
\section{Insertion sort} 
\begin{algorithm}
	\caption{``Insertion sort'' con informazione parziale} \label{insertion} 
	\begin{algorithmic}
		[1] \STATE \, \COMMENT Preparazione \STATE trova una catena \(C\) di lunghezza massima in \(P\) \STATE \, \COMMENT Ordinamento \WHILE{\(P-C\neq\emptyset\)} \STATE togli un elemento da \(P-C\) e inseriscilo in \(C\) con una ricerca binaria \ENDWHILE \RETURN \(C\) 
	\end{algorithmic}
\end{algorithm}
\begin{lemma}
	\label{maxchainlemma} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\) e sia \(C\) una catena di lunghezza massima in \(P\). Vale allora \(\left|C\right|\ge n\cdot2^{-H(\overline{P})}\). 
\end{lemma}
\begin{proof}
	È noto che l'entropia di un grafo su \(n\) vertici e dimensione massima di un insieme indipendente \(\alpha\) è maggiore o uguale a \(-\log{\frac{\alpha}{n}}\) \cite{Cardinal2005}. La tesi segue applicando questo fatto a \(G=\overline{G}(P)\).\qed 
\end{proof}
\begin{theorem}
	Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora l'algoritmo \ref{insertion} risolve il problema dell'ordinamento con informazione parziale in \(O(\log{n}\cdot\log{e(P)})\) confronti. 
\end{theorem}
\begin{proof}
	Sia \(g(P)\) il numero di confronti necessario per ordinare \(P\). È chiaro che
	\[g(P)\le \log{n}\cdot(n-|C|),\]
	inoltre per il lemma \ref{maxchainlemma}
	\[g(P)\le\log{n}\cdot(n-2^{-H(\overline{P})}n).\]
	Usando l'ovvia disuguaglianza \(1-2^{x}\le\ln{2}\cdot x\) deduciamo
	\[g(P)\le\log{n}\cdot\ln{2}\cdot nH(\overline{P}),\]
	e applicando il teorema \ref{cfjjmtheorem} abbiamo
	\[g(P)=O(\log{n}\cdot\log{e(P)}),\]
	cioè la tesi.\qed 
\end{proof}

% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ %
\section{Merge sort naive} 
\begin{algorithm}
	\caption{``Merge sort naive'' con informazione parziale} \label{naivemerge} 
	\begin{algorithmic}
		[1] \STATE \, \COMMENT Preparazione \STATE trova una decomposizione golosa di \(P\) in catene \(C_1,\dots,C_k\) \STATE \(\mathcal{C}\leftarrow\left\{C_1,\dots,C_k\right\}\) \STATE \, \COMMENT Ordinamento \WHILE{\(|\mathcal{C}|>1\)} \STATE seleziona da \(\mathcal{C}\) due catene di lunghezza minima \(C\) e \(C'\) \STATE fondi \(C\) e \(C'\) in tempo lineare, ottenendo \(C''\) \STATE cancella \(C\) e \(C'\) da \(\mathcal{C}\), aggiungi \(C''\) \ENDWHILE \RETURN l'unica catena di \(\mathcal{C}\) 
	\end{algorithmic}
\end{algorithm}
Sia \(\tilde{h}\) l'entropia di Shannon della probabilità discreta \(\left\{\frac{|C_1|}{n},\dots,\frac{|C_k|}{n}\right\}\). 
\begin{lemma}
	\label{naivemergelemma} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora l'algoritmo \ref{naivemerge} risolve il problema dell'ordinamento parziale compiendo al più \((\tilde{h}+1)n\) confronti. 
\end{lemma}
\begin{proof}
	Per fondere due catene useremo l'ovvio algoritmo lineare che a ogni passo rimuove e copia nell'output l'elemento minore fra i minimi delle catene. Nel caso peggiore tale algoritmo richiede tanti confronti quanti sono gli elementi della catena di lunghezza maggiore. La sequenza di fusioni delle catene forma un albero, detto di Huffman. È noto che l'altezza media di tale albero è maggiorata da \(\tilde{h}+1\) \cite{Cover2006}. Denotata con \(t_i\) l'altezza della catena \(C_i\), le precedenti osservazioni permettono di stimare il numero di confronti con
	\[\sum_{i=1}^{k}{t_i|C_i|}=n\sum_{i=1}^{k}{t_i\frac{|C_i|}{n}}\le n(\tilde{h}+1),\]
	cioé la tesi.\qed 
\end{proof}
\begin{theorem}
	\label{naivemergetheorem} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora, per ogni \(\varepsilon>0\), l'algoritmo \ref{naivemerge} risolve il problema dell'ordinamento parziale impiegando al più \((1+\varepsilon)\log{e(P)}+(1+\varepsilon)\left(\log{e}+\log{\left(1+\frac{1}{\varepsilon}\right)}+1\right)\cdot n\) confronti. 
\end{theorem}
\begin{proof}
	Sia \(g(P)\) il numero di confronti richiesto per ordinare \(P\). Grazie al precedente lemma otteniamo
	\[g(P)\le n(\tilde{h}+1),\]
	inoltre abbiamo 
	\begin{align}
		g(P)&\le (1+\varepsilon)nH(\overline{P})+(1+\varepsilon)n\log{\left(1+\frac{1}{\varepsilon}\right)}+n \nonumber \\
		&\le (1+\varepsilon)(\log{e(P)+\log{e\cdot n}})+(1+\varepsilon)n\log{\left(1+\frac{1}{\varepsilon}\right)}+n \nonumber \\
		&=(1+\varepsilon)\log{e(P)}+(1+\varepsilon)\left(\log{e}+\log{\left(1+\frac{1}{\varepsilon}\right)}+1\right)\cdot n, \nonumber 
	\end{align}
	dove abbiamo applicato i teoremi \ref{greedypoint} e \ref{kktheorem}.\qed 
\end{proof}

% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ %
\section{Merge con informazione parziale} 
\begin{lemma}
	\label{structurelemma} Sia \(P\) un insieme parzialmente ordinato ricoperto da due catene disgiunte \(A\) e \(B\) e sia \(G=\overline{G}(P)\) il grafo ad esso associato. Allora 
	\begin{enumerate}
		\item \(G\) è bipartito. 
		\item \(G\) è biconvesso. 
		\item Siano \(u\) e \(v\) vertici appartenenti alla stessa catena, che supporremo senza perdita di generalità essere \(A\), tali che \(u\le_{P} v\). Siano \([c_u,d_u]\) e \([c_v,d_v]\) gli intervalli dei vertici \(B\) adiacenti rispettivamente a \(u\) e \(v\). Allora \(c_u\le_{P} c_v\) e \(d_u\le_{P} d_v\), e in particolare se \(u\le_{P}w\le_{P}v\) i vertici adiacenti a \(w\) sono un intervallo contenuto in \([c_u,d_v]\). 
	\end{enumerate}
\end{lemma}
\begin{proof}
	TODO\qed 
\end{proof}
Per evitare casi degeneri denoteremo nel seguito con \(\text{STAB}^{*}(G)\) i punti di \(\text{STAB}(G)\) di coordinate tutte positive. Infatti se così non fosse la funzione obiettivo dell'equazione \ref{eq:entropythree} non sarebbe definita.

Nel caso di grafi bipartiti possiamo semplificare il lemma \ref{chvatallemma}. Abbiamo allora che
\[\text{STAB}(G)=\left\{x\in \mathbb{R}^V:\quad x_u + x_v\le 1 \text{ per ogni } uv\in E,\quad 0\le x_v\le 1 \text{ per ogni } v\in V\right\}.\]

\begin{definition}
	Diciamo che un arco \(uv\) è \emph{stretto} rispetto ad \(x\in\text{STAB}(G)\) se vale \(x_u+x_v=1\). Denotiamo con \(G(x)\) il grafo i cui vertici siano gli stessi di \(G\) e i cui archi siano stretti rispetto ad \(x\). 
\end{definition}
\begin{definition}
	Siano \(uv\) e \(u'v'\) archi di \(G\) tali che \(u,u'\in A\) e \(v,v'\in B\). Diciamo che \emph{si incrociano} se \(u<_{P}u'\) e \(v'<_{P}v\) oppure se \(u'<_{P}u\) e \(v<_{P}v'\). 
\end{definition}
\begin{lemma}
	\label{crossinglemma} Sia \(P\) un insieme parzialmente ordinato ricoperto da due catene disgiunte \(A\) e \(B\) e sia \(G=\overline{G}(P)\) il grafo ad esso associato. Sia \(x\) un punto di \(\text{STAB}(G)\) e siano \(uv\) e \(u'v'\) archi stretti rispetto ad \(x\) tali inoltre che \(u,u'\in A\) e \(v,v'\in B\). Se \(uv\) e \(u'v'\) si incrociano allora sia \(u'v\) sia \(uv'\) sono archi di \(G\), entrambi stretti rispetto ad \(x\). 
\end{lemma}
\begin{proof}
	Dal lemma \ref{structurelemma} \((3)\) segue che \(u'v\) e \(uv'\) sono archi di \(G\). Supponiamo per assurdo che \(uv'\) non sia stretto. Avremmo allora: 
	\begin{align}
		x_v &= 1-x_u &\text{(poich\'e } uv \text{ \`e stretto)}\nonumber \\
		&> x_{v'} &\text{(poich\'e } uv' \text{ non \`e stretto)}\nonumber \\
		&= 1-x_{u'} &\text{(poich\'e } u'v' \text{ \`e stretto)}\nonumber \\
		&\ge x_{v}, &\text{(poich\'e } u'v \text{ \`e un arco e per il lemma \ref{chvatallemma})}\nonumber 
	\end{align}
	chiaramente un assurdo. Possiamo procedere analogamente per \(u'v\), da cui la tesi.\qed 
\end{proof}
\begin{definition}
	Diciamo che \(x\in\text{STAB}^{*}(G)\) è \emph{localmente ottimo} se per ogni componente connessa \(K\) di \(G(x)\) valgono
	\[x_u=\frac{|A\cap K|}{|K|}\quad\text{per ogni}\,u\in A\cap K\qquad\text{e}\qquad x_v=\frac{|B\cap K|}{|K|}\quad\text{per ogni}\,v\in B\cap K.\]
	Diciamo che \(K\) è \emph{bilanciata} se per essa valgono le precedenti condizioni di ottimalità, \emph{sbilanciata} altrimenti. 
\end{definition}
\begin{definition}
	Sia \(x\in\text{STAB}^*(G)\). Una componente connessa \(K\) di \(G(x)\) è detta banale se consiste di un unico vertice, \emph{non banale} altrimenti. Inoltre chiamiamo \emph{libera} una componente che sia banale e sbilanciata. 
\end{definition}
\begin{definition}
	Sia \(x\in\text{STAB}^{*}(G)\). Una componente connessa \(L\) di \(G(x)\) è detta \emph{incastonata} in un'altra componente connessa \(K\) se esistono un vertice \(w\in L\) e due vertici \(u,u^{''}\in K\) tutti appartenenti ad un'unica catena e tali inoltre che \(u\le_{P}w\le_{P}u^{''}\). 
\end{definition}
La relazione d'ordine \(\le_{P}\) induce una relazione d'ordine sui sottoinsiemi dell'insieme di sostegno. Diciamo che \(S\le_{P}T\) se \(S\) e \(T\) sono sottoinsiemi tali che \(u\le_{P}v\) per ogni \(u\in S\) e per ogni \(v\in T\). 
\begin{lemma}
	\label{inlaylemma} Sia \(P\) un insieme parzialmente ordinato ricoperto da due catene disgiunte \(A\) e \(B\) e sia \(G=\overline{G}(P)\) il grafo ad esso associato. Dato \(x\in\text{STAB}^{*}(G)\) allora 
	\begin{enumerate}
		\item se in \(G(x)\) una componente connessa \(L\) è incastonata in \(K\) allora \(L\) è libera. 
		\item se \(K\) ed \(L\) sono componenti connesse non banali di \(G(x)\) allora vale \(K\le_{P}L\) oppure \(L\le_{P}K\). 
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}
		\item Supponiamo per assurdo che \(L\) non sia libera ma sia incastonata in \(K\). Siano allora \(w\in L\) e \(u,u^{''}\in K\) come nella definizione. Senza perdita di generalità possiamo assumere \(w,u',u^{''}\in A\) e che \(u'\not\in K\) se \(u\le_{P}u'\le_{P}u^{''}\). \(K\) \`e una componente connessa che contiene sia \(u\) sia \(u^{''}\), dunque esiste \(v\in K\cap B\) adiacente ad entrambi. Per il lemma \ref{structurelemma} \((3)\) abbiamo che \(vw\) \`e un arco di \(G\), ma poich\'e \(K\) ed \(L\) sono componenti connesse distinte \(vw\) non \`e un arco di \(G(x)\). Se \(L\) non fosse banale allora esisterebbe un arco di \(L\) incidente in \(w\) che incrocia \(uv\) o \(u^{''}v\), dunque per il lemma \ref{crossinglemma} avremmo che \(vw\) \`e stretto, assurdo. Se invece \(L\) fosse bilanciata avremmo \(x_w+x_v=1+x_v>1\), contro il lemma \ref{chvatallemma}. 
		\item Supponiamo per assurdo che non valga nessuna delle due disuguaglianze. Per il punto precedente, essendo non banali, \(K\) e \(L\) non possono essere incastonate l'una nell'altra. Senza perdita di generalità possiamo allora assumere che valgano
		\[K\cap A\le_{P}L\cap A\qquad\text{ e }\qquad L\cap B\le_{P}K\cap B.\]
		Siano quindi \(u,u'\in A\) e \(v,v'\in B\) tali che \(uv\) e \(u'v'\) siano archi di \(G\). Dunque tali archi si incrociano, perciò in particolare \(uv'\) è un arco di \(G\). Ma allora \(K\) ed \(L\) sarebbero la stessa componente connessa, una contraddizione.\qed 
	\end{enumerate}
\end{proof}
\begin{definition}
	Sia \(x\in \text{STAB}^*(G)\) e sia \(K\) una componente connessa di \(G(x)\). Chiamiamo \emph{scarto} di \(K\) il reale \(\sigma\) che renda minimo
	\[\left|x_v+\sigma-\frac{|A\cap K|}{|K|}\right|\]
	per ogni \(v\in A\cap K\), e tale inoltre che, se
	\[ x' = 
	\begin{cases}
		x_v + \sigma \\
		x_v - \sigma 
	\end{cases}
	\]
\end{definition}
\begin{algorithm}
	\caption{Ribilanciamento} \label{rebalance} 
	\begin{algorithmic}
		[1] \WHILE{esiste una componente sbilanciata \(K\) di \(G(x)\)} 
		\STATE calcola lo scarto \(\sigma\) di \(K\)
		\STATE poni \(x'_v := x_v+\sigma\) per \(v\in A\cap K\), \(x'_v := x_v-\sigma\) per \(v\in B\cap K\)\ENDWHILE \RETURN \(x'\) 
	\end{algorithmic}
\end{algorithm}
\begin{lemma}
	\label{touchinglemma} TODO 
\end{lemma}
\begin{proof}
	TODO\qed 
\end{proof}
\begin{definition}
	Sia \(x\in\text{STAB}^{*}(G)\). Diciamo che una componente connessa \(K\) di \(G(x)\) è \emph{rossa} se si ha \(|A\cap K|\ge|B\cap K|\), altrimenti diciamo che \(K\) è \emph{blu}. 
\end{definition}
\begin{lemma}
	\label{consistentlemma} TODO 
\end{lemma}
\begin{proof}
	TODO\qed 
\end{proof}
\begin{lemma}
	\label{hwanglinlemma} Siano \(X\) e \(Y\) due catene disgiunte. Supponiamo che \(|X|\ge|Y|\). Allora il numero di confronti richiesto dall'algoritmo di Hwang-Lin è maggiorato da \(|Y|\log(\frac{4|X|}{|Y|})\). 
\end{lemma}
\begin{proof}
	È noto che l'algoritmo di Hwang-Lin compie al più
	\[|Y|\left(1+\left\lfloor{\log{\frac{X}{Y}}}\right\rfloor\right)+\left\lfloor\frac{|X|}{2^{\left\lfloor\log{\frac{|X|}{|Y|}}\right\rfloor}}\right\rfloor-1\]
	confronti \cite{Hwang1972}. Sia allora \(\xi\in\left[0,1\right)\) tale che
	\[\left\lfloor\log{\frac{|X|}{|Y|}}\right\rfloor=\log{\frac{|X|}{|Y|}}-\xi.\]
	È facile verificare che per \(\xi\in\left[0,1\right)\) vale la disuguaglianza
	\[1-\xi+2^{\xi}\le 2.\]
	Semplici passaggi algebrici danno
	\[\frac{|X|}{2^{\left\lfloor\log{\frac{|X|}{|Y|}}\right\rfloor}}=\frac{|X|}{2^{\log{\frac{|X|}{|Y|}}-\xi}}=\frac{|X|}{2^{\log{\frac{|X|}{|Y|}}}}\cdot 2^{\xi}=|Y|\cdot 2^{\xi}.\]
	Possiamo infine mettere insieme le precedenti due equazioni per ottenere 
	\begin{align}
		|Y|\left(1+\left\lfloor{\log{\frac{X}{Y}}}\right\rfloor\right)+\left\lfloor\frac{|X|}{2^{\left\lfloor\log{\frac{|X|}{|Y|}}\right\rfloor}}\right\rfloor-1&\le|Y|\left(1-\xi+\log{\frac{|X|}{|Y|}}+2^{\xi}\right) \nonumber \\
		&\le |Y|\left(\log{\frac{|X|}{|Y|}}+2\right) \nonumber \\
		&= |Y|\left(\log{\frac{4|X|}{|Y|}}\right), \nonumber 
	\end{align}
	cioé la tesi.\qed 
\end{proof}
\begin{definition}
	Sia \(K\) una componente connessa di \(G(x)\). Se \(K\) è rossa chiamiamo \(A\cap K\) \emph{catena maggiore} e \(B\cap K\) \emph{catena minore}. Se \(K\) è blu il contrario. 
\end{definition}
\begin{definition}
	Sia \(K\) una componente connessa di \(G(x)\). Diciamo che \(K\) è \emph{buona} se ogni arco di \(G\) che possiede un estremo nella catena minore di \(K\) ha l'altro estremo nella catena maggiore oppure in una componente connessa di colore opposto. 
\end{definition}
\begin{lemma}
	\label{goodlemma} Sia \(x\in \text{STAB}(G)\) localmente ottimo. Se \(G(x)\) possiede almeno una componente rossa non banale allora una di esse è buona. 
\end{lemma}
\begin{proof}
	Sia \(K\) una componente connessa rossa non banale tale che \(\frac{|A\cap K|}{|K|}\) sia minimo. Vogliamo dimostrare che \(K\) è buona. Sia \(v\in B\cap K\) e sia \(w\) adiacente a \(v\) in \(G\) ma non in \(G(x)\). Per definizione l'arco di estremi \(v\) e \(w\) non è stretto, quindi \(x_v+x_w<1\). In particolare \(x_w<1\), quindi \(w\) appartiene ad una qualche componente connessa \(L\) non banale. Se per assurdo \(L\) fosse rossa per ipotesi \(\frac{|A\cap L|}{|L|}\ge\frac{|A\cap K|}{|K|}\), dunque per ottimalità di \(x\) avremmo
	\[x_v+x_w=\frac{|B\cap K|}{|K|}+\frac{|A\cap L|}{|L|}\ge\frac{|B\cap K|}{|K|}+\frac{|A\cap K|}{|K|}\ge 1\]
	da cui dedurremmo che l'arco di estremi \(v\) e \(w\) è stretto, una contraddizione. Segue quindi che \(L\) è blu oppure non esiste \(w\) adiacente a \(v\) in \(G\) ma non in \(G(x)\), cioè la tesi.\qed 
\end{proof}
È immediato osservare che la precedente dimostrazione si applica, \emph{mutatis mutandis}, all'insieme delle componenti blu non banali. Pertanto in analoghe ipotesi esiste una componente blu che sia buona. 
\begin{lemma}
	\label{evolutionlemma} TODO 
\end{lemma}
\begin{proof}
	TODO\qed 
\end{proof}
\begin{lemma}
	\label{finallemma} TODO 
\end{lemma}
\begin{proof}
	TODO\qed 
\end{proof}
\begin{algorithm}
	\caption{``Merge'' con informazione parziale} \label{merge} 
	\begin{algorithmic}
		[1] \STATE TODO 
	\end{algorithmic}
\end{algorithm}
\begin{theorem}
	\label{mergetheorem} Sia \(P\) un insieme parzialmente ordinato ricoperto da due catene disgiunte \(A\) e \(B\) e sia \(G=\overline{G}(P)\) il grafo ad esso associato. Allora l'algoritmo \ref{merge} fonde \(A\) e \(B\) impiegando al più \(6\log{e(P)}\) confronti. 
\end{theorem}
\begin{proof}
	TODO\qed 
\end{proof}

% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ %
\section{Merge sort} 
\begin{algorithm}
	\caption{``Merge sort'' con informazione parziale} \label{mergesort} 
	\begin{algorithmic}
		[1] \STATE trova una catena \(A\) di lunghezza massima in \(P\) \STATE applica l'algoritmo \ref{naivemerge} a \(P-A\), ottenendo una catena \(B\) \STATE applica l'algoritmo \ref{merge} all'ordine parziale corrente \(P'\) \RETURN la catena risultante 
	\end{algorithmic}
\end{algorithm}
\begin{theorem}
	\label{mergesorttheorem} Sia \(P\) un insieme parzialmente ordinato di cardinalità \(n\). Allora l'algoritmo \ref{mergesort} risolve il problema dell'ordinamento con informazione parziale impiegando al più \(c \log{e(P)}\) confronti, dove \(c\approx 15.08\). 
\end{theorem}
\begin{proof}
	Sia \(g(P)\) il numero di confronti necessario ad ordinare \(P\). Per il lemma \ref{maxchainlemma} abbiamo \(|A|\ge n\cdot 2^{-H(\overline{P})}\), dunque
	\[|B|=|P-A|\le n\left(1-2^{-H(\overline{P})}\right)\le\ln{2}\cdot nH(\overline{P}),\]
	in cui abbiamo usato l'ovvia disuguaglianza \(1-2^{x}\le\ln{2}\cdot x\). Grazie ai teoremi \ref{naivemergetheorem} e \ref{mergetheorem} possiamo maggiorare il numero di confronti compiuti con
	\[g(P)\le(1+\varepsilon)\log{e(P-A)}+\left((1+\varepsilon)\left(\log{e}+\log{\left(1+\frac{1}{\varepsilon}\right)}\right)+1\right)|P-A|+6\log{e(P')}\]
	e, per la disuguaglianza appena dimostrata,
	\[ g(P)\le(1+\varepsilon)\log{e(P)}+\left((1+\varepsilon)\left(1+\ln{\left(1+\frac{1}{\varepsilon}\right)}\right)+\ln{2}\right)nH(\overline{P})+6\log{e(P')}.\]
	Possiamo quindi applicare il teorema \ref{cfjjmtheorem} ed ottenere la stima
	\[g(P)\le\left(1+\varepsilon+2\left((1+\varepsilon)\left(1+\ln{\left(1+\frac{1}{\varepsilon}\right)}\right)+\ln{2}\right)+6\right)\log{e(P)}.\]
	Infine, ponendo \(\varepsilon\approx 0.351198\), abbiamo
	\[g(P)\le c\log{e(P)}\nonumber\]
	dove \(c\approx 15.08\), cioè la tesi.\qed 
\end{proof}
